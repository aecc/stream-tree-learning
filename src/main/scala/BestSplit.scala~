import org.apache.spark.SparkContext._
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import spark.util.Vector


object BestSplit{

	def bestSplit(inputset: RDD[(Int, Vector[Int])], attribset: List[String]): (Set) = {
	
		val rdd_size = inputset.count
		var i_gain = Set[Any]()
		var a = 0

		for(a <- 0 until attribset.size) {
		
			
		
		}
		
	}
	

	def mapSamples(inputset: RDD[(Int, Vector[Int])] {
	
		val featureAndValue = inputset.map{
			case(image_id,(numberOfWords_title,_,_,_))=> {
				
				(image_id,numberOfWords_title)
			
			}
		
		}
	
	
	}




	
}
